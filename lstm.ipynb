{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "409.46832614502375\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 381.65 248.518125\" width=\"381.65pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-07T19:20:08.486121</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 248.518125 \r\nL 381.65 248.518125 \r\nL 381.65 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 39.65 224.64 \r\nL 374.45 224.64 \r\nL 374.45 7.2 \r\nL 39.65 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m93e09084a9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.868182\" xlink:href=\"#m93e09084a9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(51.686932 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.845176\" xlink:href=\"#m93e09084a9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5000 -->\r\n      <g transform=\"translate(97.120176 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.822171\" xlink:href=\"#m93e09084a9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10000 -->\r\n      <g transform=\"translate(148.915921 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.799165\" xlink:href=\"#m93e09084a9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15000 -->\r\n      <g transform=\"translate(203.892915 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"274.776159\" xlink:href=\"#m93e09084a9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20000 -->\r\n      <g transform=\"translate(258.869909 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"329.753154\" xlink:href=\"#m93e09084a9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25000 -->\r\n      <g transform=\"translate(313.846904 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m7d2eba5431\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m7d2eba5431\" y=\"218.668831\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(26.2875 222.46805)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m7d2eba5431\" y=\"172.160619\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(7.2 175.959838)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m7d2eba5431\" y=\"125.652407\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2000 -->\r\n      <g transform=\"translate(7.2 129.451626)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m7d2eba5431\" y=\"79.144195\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3000 -->\r\n      <g transform=\"translate(7.2 82.943414)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m7d2eba5431\" y=\"32.635983\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4000 -->\r\n      <g transform=\"translate(7.2 36.435201)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#pbb88fb4f50)\" d=\"M 54.868182 184.400312 \r\nL 55.209039 186.832824 \r\nL 55.220035 186.47356 \r\nL 55.406956 185.600266 \r\nL 55.417952 185.87271 \r\nL 55.670846 187.803815 \r\nL 55.681841 187.177351 \r\nL 56.088671 185.910955 \r\nL 56.099666 186.373369 \r\nL 56.319574 185.022546 \r\nL 56.88034 187.54313 \r\nL 56.891335 188.516171 \r\nL 57.122239 191.274767 \r\nL 57.133234 191.178625 \r\nL 57.276174 191.40997 \r\nL 57.28717 191.740795 \r\nL 57.562055 190.568315 \r\nL 57.57305 190.656804 \r\nL 57.693999 191.939143 \r\nL 57.704995 191.615004 \r\nL 57.847935 190.601923 \r\nL 57.85893 190.69201 \r\nL 58.287751 193.011894 \r\nL 58.298746 193.848502 \r\nL 58.441686 194.181753 \r\nL 58.452682 194.946679 \r\nL 58.650599 192.385411 \r\nL 58.925484 192.375541 \r\nL 58.936479 192.979872 \r\nL 59.3653 191.466535 \r\nL 59.376295 191.5498 \r\nL 59.519236 192.368331 \r\nL 59.530231 191.63513 \r\nL 60.20095 189.714057 \r\nL 60.211946 191.179568 \r\nL 60.376877 191.270731 \r\nL 60.387872 191.186762 \r\nL 60.882665 190.220934 \r\nL 60.89366 190.311483 \r\nL 61.674334 190.964186 \r\nL 61.685329 191.515688 \r\nL 61.916232 194.324953 \r\nL 61.927228 194.212173 \r\nL 62.378039 194.213511 \r\nL 62.389035 195.975389 \r\nL 62.905818 196.414276 \r\nL 62.916814 194.531077 \r\nL 63.21369 195.784424 \r\nL 63.224685 195.650058 \r\nL 63.961377 195.432589 \r\nL 63.972372 194.713874 \r\nL 64.159294 194.333176 \r\nL 64.170289 193.971117 \r\nL 64.775036 193.8892 \r\nL 64.852004 194.841124 \r\nL 65.104898 197.455816 \r\nL 65.170871 197.145296 \r\nL 65.181866 196.346881 \r\nL 65.467746 197.92876 \r\nL 65.522723 198.751424 \r\nL 65.533719 197.542622 \r\nL 65.742631 197.621812 \r\nL 65.753627 197.081889 \r\nL 65.98453 197.592185 \r\nL 65.995525 198.096686 \r\nL 66.468328 198.599226 \r\nL 66.479323 199.052611 \r\nL 67.150042 198.068365 \r\nL 67.161038 197.783984 \r\nL 67.34796 197.554593 \r\nL 67.358955 197.936953 \r\nL 67.63384 197.307522 \r\nL 67.644835 196.328151 \r\nL 68.095647 197.301027 \r\nL 68.106642 196.618724 \r\nL 68.601435 197.069 \r\nL 68.61243 197.250239 \r\nL 69.294145 195.772874 \r\nL 69.700975 197.565915 \r\nL 69.71197 198.331425 \r\nL 70.074818 198.279243 \r\nL 70.085814 198.430789 \r\nL 70.844496 198.538782 \r\nL 70.855492 198.682147 \r\nL 71.438248 196.262962 \r\nL 71.449243 196.632574 \r\nL 71.614174 196.608444 \r\nL 71.62517 195.067884 \r\nL 72.471815 197.738733 \r\nL 72.482811 197.319346 \r\nL 73.197512 197.07121 \r\nL 73.208507 197.357153 \r\nL 73.813254 196.290214 \r\nL 73.824249 196.631664 \r\nL 75.033743 197.329523 \r\nL 75.407587 199.135904 \r\nL 76.45215 199.705536 \r\nL 76.463145 199.051935 \r\nL 77.529699 201.647783 \r\nL 77.540694 201.986309 \r\nL 77.738611 201.613382 \r\nL 77.991506 202.434401 \r\nL 78.002501 203.73695 \r\nL 78.717202 203.896115 \r\nL 78.728197 205.812171 \r\nL 79.135027 206.253835 \r\nL 79.146023 205.360015 \r\nL 79.618825 205.186698 \r\nL 79.62982 204.523026 \r\nL 80.344521 204.915022 \r\nL 80.355516 204.81912 \r\nL 80.938273 205.964128 \r\nL 80.949268 205.324429 \r\nL 81.334107 206.250862 \r\nL 81.345102 205.964784 \r\nL 81.817904 205.641006 \r\nL 81.8289 206.070686 \r\nL 83.005408 205.038137 \r\nL 83.016403 205.748591 \r\nL 83.973003 204.974769 \r\nL 83.983998 204.997232 \r\nL 84.72069 205.491951 \r\nL 84.731685 207.15721 \r\nL 85.27046 208.534806 \r\nL 85.281455 208.171742 \r\nL 85.776248 209.743627 \r\nL 85.787243 209.231986 \r\nL 86.787825 210.488922 \r\nL 86.79882 209.561875 \r\nL 87.183659 208.62939 \r\nL 87.194655 208.707496 \r\nL 87.590489 208.268972 \r\nL 88.217227 209.446427 \r\nL 88.228222 209.082763 \r\nL 88.920932 208.548717 \r\nL 89.063872 207.362231 \r\nL 89.470702 207.528757 \r\nL 89.481697 206.99522 \r\nL 90.130426 208.979268 \r\nL 90.141421 208.716002 \r\nL 91.05404 208.157328 \r\nL 91.065035 207.893421 \r\nL 93.253119 208.366506 \r\nL 93.264115 207.718821 \r\nL 94.37465 209.009776 \r\nL 94.385645 209.568988 \r\nL 95.254282 209.119777 \r\nL 95.265277 208.591657 \r\nL 96.232872 208.969342 \r\nL 99.190635 209.268101 \r\nL 99.20163 209.677125 \r\nL 101.27976 208.720999 \r\nL 101.290756 208.526807 \r\nL 103.258932 208.985857 \r\nL 103.269928 210.076785 \r\nL 104.534399 210.722887 \r\nL 104.545394 211.647719 \r\nL 106.106741 213.464916 \r\nL 107.019359 213.305306 \r\nL 107.030354 213.109663 \r\nL 108.28383 213.279756 \r\nL 109.284411 214.394491 \r\nL 109.295406 214.573771 \r\nL 109.52631 214.191964 \r\nL 109.537305 214.323557 \r\nL 110.955711 214.449133 \r\nL 110.966707 214.756364 \r\nL 112.099233 214.373056 \r\nL 112.110228 214.199642 \r\nL 113.66058 212.499091 \r\nL 113.671575 211.663292 \r\nL 114.309308 212.832736 \r\nL 116.277485 211.691728 \r\nL 116.28848 211.439085 \r\nL 117.289061 211.152639 \r\nL 117.300057 211.075578 \r\nL 119.158279 213.116566 \r\nL 119.169274 213.07273 \r\nL 119.686058 212.512521 \r\nL 119.697054 212.781287 \r\nL 124.194172 213.691839 \r\nL 124.205167 213.564159 \r\nL 125.953436 213.736956 \r\nL 125.964431 213.355053 \r\nL 129.153097 211.929147 \r\nL 130.175669 211.189867 \r\nL 130.186664 212.19347 \r\nL 135.387488 212.29929 \r\nL 135.398483 212.038123 \r\nL 136.772908 212.141665 \r\nL 136.783903 212.381188 \r\nL 139.917592 211.872801 \r\nL 139.928588 210.893135 \r\nL 140.489353 211.02169 \r\nL 140.500348 211.220395 \r\nL 141.368985 210.588303 \r\nL 141.37998 210.348511 \r\nL 144.007881 211.17505 \r\nL 144.12883 210.46445 \r\nL 146.998629 211.235131 \r\nL 147.009624 211.014135 \r\nL 148.757893 210.577579 \r\nL 148.768888 210.254298 \r\nL 149.780465 210.556368 \r\nL 150.759055 209.512 \r\nL 150.770051 209.819841 \r\nL 151.539729 206.343114 \r\nL 151.880586 208.077308 \r\nL 151.891582 207.758742 \r\nL 152.430356 206.678481 \r\nL 152.441351 206.910475 \r\nL 153.331979 207.134673 \r\nL 153.342974 207.263209 \r\nL 154.18962 206.915475 \r\nL 154.200615 206.177716 \r\nL 155.377123 206.034557 \r\nL 155.388118 206.585385 \r\nL 156.190782 206.059069 \r\nL 156.201778 206.602607 \r\nL 156.828516 207.579628 \r\nL 156.839511 207.32306 \r\nL 157.400276 207.093585 \r\nL 157.411272 207.261854 \r\nL 158.345881 207.54364 \r\nL 158.356876 207.293466 \r\nL 159.15954 206.491168 \r\nL 159.170536 206.804162 \r\nL 159.368453 206.113856 \r\nL 160.347043 206.141918 \r\nL 160.358039 206.35383 \r\nL 160.940795 205.808989 \r\nL 160.95179 205.428694 \r\nL 161.171698 204.340771 \r\nL 162.238252 204.063336 \r\nL 162.381192 202.662286 \r\nL 162.524132 203.530089 \r\nL 162.535128 204.510372 \r\nL 163.029921 204.627299 \r\nL 163.040916 204.325977 \r\nL 163.491727 204.446984 \r\nL 163.502723 205.178693 \r\nL 164.294391 205.023273 \r\nL 165.294973 205.129643 \r\nL 165.305968 204.501944 \r\nL 165.723793 203.858102 \r\nL 166.196595 205.423714 \r\nL 166.207591 205.881728 \r\nL 167.054237 206.366364 \r\nL 167.065232 207.708936 \r\nL 167.406089 208.305671 \r\nL 167.417085 207.763215 \r\nL 167.845905 209.666236 \r\nL 167.856901 209.090147 \r\nL 168.032827 208.556702 \r\nL 168.571602 208.31612 \r\nL 168.582597 208.169806 \r\nL 170.17693 208.791583 \r\nL 170.187925 208.571975 \r\nL 170.484801 208.42356 \r\nL 170.495796 209.07418 \r\nL 171.716286 208.703126 \r\nL 171.727281 208.561647 \r\nL 173.805411 208.002217 \r\nL 173.816407 208.814714 \r\nL 174.926942 208.843737 \r\nL 174.937938 208.594199 \r\nL 175.520694 210.172856 \r\nL 175.531689 209.986668 \r\nL 177.433893 209.649152 \r\nL 177.444889 209.500875 \r\nL 179.962835 210.910682 \r\nL 179.97383 210.512629 \r\nL 181.601149 210.486189 \r\nL 181.854043 210.146228 \r\nL 181.865039 210.429579 \r\nL 183.96516 209.92731 \r\nL 183.976155 209.595393 \r\nL 184.690856 208.613583 \r\nL 184.701852 209.118421 \r\nL 184.80081 208.699466 \r\nL 184.811806 208.249855 \r\nL 186.142249 209.450615 \r\nL 186.153244 210.908307 \r\nL 187.40672 210.701521 \r\nL 188.407301 210.243878 \r\nL 188.418297 210.503235 \r\nL 189.550823 210.578377 \r\nL 189.561818 210.296839 \r\nL 190.078602 210.078039 \r\nL 190.089597 209.797357 \r\nL 191.881847 210.323794 \r\nL 191.892843 210.556326 \r\nL 192.37664 210.43716 \r\nL 194.300835 211.076692 \r\nL 194.31183 210.182982 \r\nL 199.094829 209.96245 \r\nL 199.105824 209.665994 \r\nL 201.975623 211.301825 \r\nL 201.986619 211.473179 \r\nL 202.415439 212.266994 \r\nL 202.426435 211.793299 \r\nL 203.009191 211.809008 \r\nL 203.020186 211.532261 \r\nL 207.231424 211.937597 \r\nL 207.242419 211.786055 \r\nL 208.330964 212.53981 \r\nL 208.341959 212.745351 \r\nL 211.574607 212.646323 \r\nL 214.708295 212.046671 \r\nL 214.719291 212.134221 \r\nL 215.719872 211.63464 \r\nL 215.730867 211.150366 \r\nL 216.555522 210.835156 \r\nL 216.566518 210.567197 \r\nL 217.226242 211.003254 \r\nL 219.304372 210.975605 \r\nL 219.315367 210.62755 \r\nL 221.525443 209.596076 \r\nL 221.536438 209.092351 \r\nL 222.185167 208.253292 \r\nL 222.196162 208.782743 \r\nL 223.504614 206.737833 \r\nL 223.51561 206.455396 \r\nL 224.120357 205.489397 \r\nL 224.131352 206.535819 \r\nL 224.934016 206.392386 \r\nL 224.945012 205.275513 \r\nL 225.417814 206.780539 \r\nL 225.428809 206.164612 \r\nL 227.045133 207.010683 \r\nL 227.056128 208.153443 \r\nL 227.814811 208.09163 \r\nL 227.825806 208.388984 \r\nL 229.354167 207.852992 \r\nL 229.497107 206.919236 \r\nL 230.255789 209.694275 \r\nL 230.266785 209.246093 \r\nL 230.54167 209.270412 \r\nL 230.552665 209.002582 \r\nL 230.915513 210.686637 \r\nL 230.926509 212.423385 \r\nL 231.289357 212.985003 \r\nL 231.300352 212.403883 \r\nL 232.136003 213.592564 \r\nL 233.070611 212.241779 \r\nL 233.081607 212.627931 \r\nL 234.840871 212.222834 \r\nL 235.082769 212.806569 \r\nL 237.512753 211.831712 \r\nL 237.523748 211.057381 \r\nL 237.963564 210.674016 \r\nL 239.261021 211.476442 \r\nL 240.129658 211.277861 \r\nL 240.140653 210.858713 \r\nL 241.075262 210.064363 \r\nL 241.086257 210.106856 \r\nL 241.229197 210.279228 \r\nL 241.240193 210.751447 \r\nL 243.846102 208.933237 \r\nL 243.857098 208.740697 \r\nL 245.176546 209.169828 \r\nL 246.375044 208.75825 \r\nL 246.386039 210.09027 \r\nL 247.9144 209.293695 \r\nL 247.925395 209.108971 \r\nL 250.201443 208.966234 \r\nL 250.212438 209.42776 \r\nL 251.564872 208.629054 \r\nL 251.575868 208.138953 \r\nL 252.59844 207.130563 \r\nL 252.609435 207.527583 \r\nL 254.995437 207.275661 \r\nL 255.006432 207.599853 \r\nL 256.072986 207.987143 \r\nL 256.083981 207.839656 \r\nL 256.710719 207.808934 \r\nL 256.721714 208.163732 \r\nL 258.338038 207.363038 \r\nL 258.349033 207.600853 \r\nL 259.382601 208.373737 \r\nL 262.274391 207.504889 \r\nL 262.285386 207.36468 \r\nL 264.913287 207.784314 \r\nL 264.924282 208.138609 \r\nL 266.441647 205.654 \r\nL 266.65056 204.932418 \r\nL 266.661555 204.227092 \r\nL 268.03598 203.45221 \r\nL 268.046975 203.112064 \r\nL 268.387833 202.082872 \r\nL 268.398828 202.218265 \r\nL 268.58575 200.122744 \r\nL 268.596745 201.529022 \r\nL 269.069547 200.354733 \r\nL 269.080543 200.499873 \r\nL 270.015152 200.20671 \r\nL 270.026147 200.624781 \r\nL 271.312609 200.652479 \r\nL 271.323604 200.790372 \r\nL 271.664462 200.264956 \r\nL 271.675457 198.619802 \r\nL 272.544093 198.627805 \r\nL 272.555089 198.975115 \r\nL 272.654047 198.620443 \r\nL 272.665043 198.501559 \r\nL 273.599652 199.843208 \r\nL 273.610647 200.183774 \r\nL 274.501274 200.416122 \r\nL 274.51227 199.748294 \r\nL 274.721182 200.184348 \r\nL 274.732178 200.800694 \r\nL 275.040049 200.945249 \r\nL 275.919681 199.987601 \r\nL 276.084612 199.554408 \r\nL 276.095607 199.177036 \r\nL 276.524428 198.004111 \r\nL 276.557414 196.370208 \r\nL 276.85429 197.076227 \r\nL 276.865285 197.865013 \r\nL 277.656954 201.729677 \r\nL 277.722926 203.537761 \r\nL 277.898853 202.289564 \r\nL 278.459618 202.787123 \r\nL 278.470613 203.217342 \r\nL 278.965406 201.453559 \r\nL 278.976402 201.671057 \r\nL 279.427213 201.477776 \r\nL 279.438209 201.291183 \r\nL 279.779066 200.838222 \r\nL 279.790061 201.936792 \r\nL 279.88902 201.743994 \r\nL 279.900015 201.344603 \r\nL 280.416799 201.436295 \r\nL 280.427794 201.806529 \r\nL 281.120505 200.400371 \r\nL 281.1315 200.956458 \r\nL 281.340413 200.810748 \r\nL 281.351408 200.371843 \r\nL 282.0881 202.763632 \r\nL 282.099095 202.521745 \r\nL 282.63787 203.51844 \r\nL 282.648865 202.748214 \r\nL 282.989722 202.498695 \r\nL 283.000718 202.20074 \r\nL 284.386138 201.966795 \r\nL 285.232784 202.759942 \r\nL 285.243779 202.235551 \r\nL 285.980471 202.557388 \r\nL 285.991466 203.017408 \r\nL 286.596213 202.30249 \r\nL 286.607209 201.689732 \r\nL 287.60779 200.786264 \r\nL 287.618785 200.814069 \r\nL 287.794712 201.155489 \r\nL 289.411035 201.335073 \r\nL 289.422031 201.493834 \r\nL 289.850851 200.738384 \r\nL 289.861847 200.390772 \r\nL 290.026778 200.083028 \r\nL 290.037773 199.292837 \r\nL 291.148308 199.822043 \r\nL 291.159304 200.122938 \r\nL 292.511738 200.905704 \r\nL 292.522733 200.515201 \r\nL 293.105489 200.649666 \r\nL 293.116485 201.371698 \r\nL 293.875167 199.378007 \r\nL 293.886163 198.715189 \r\nL 294.292992 197.03461 \r\nL 294.303988 198.360547 \r\nL 294.666836 197.554714 \r\nL 294.677831 197.654764 \r\nL 294.798781 197.623818 \r\nL 294.809776 198.088982 \r\nL 295.568459 196.917812 \r\nL 295.579454 197.328562 \r\nL 296.096238 196.739194 \r\nL 296.107233 197.213097 \r\nL 296.646008 197.758328 \r\nL 296.657003 197.1751 \r\nL 297.393695 196.23638 \r\nL 297.40469 197.133393 \r\nL 298.108396 193.353156 \r\nL 298.295318 193.74135 \r\nL 298.306313 192.622023 \r\nL 298.471244 191.52536 \r\nL 298.482239 190.641998 \r\nL 298.955042 190.661526 \r\nL 298.966037 190.140843 \r\nL 299.394857 195.031216 \r\nL 299.405853 194.431556 \r\nL 299.735715 195.077596 \r\nL 299.922637 193.243969 \r\nL 299.933632 193.930619 \r\nL 300.395439 190.447256 \r\nL 300.637338 190.461158 \r\nL 300.70331 191.342191 \r\nL 301.176112 190.477713 \r\nL 301.187108 190.026454 \r\nL 301.374029 190.468443 \r\nL 301.385025 191.413971 \r\nL 301.6819 191.494788 \r\nL 301.692896 191.014888 \r\nL 302.220675 190.805459 \r\nL 302.737459 193.192798 \r\nL 302.748454 192.014693 \r\nL 303.133293 192.855677 \r\nL 303.144289 193.593414 \r\nL 304.034916 191.038718 \r\nL 304.045911 192.04358 \r\nL 304.738621 189.785531 \r\nL 304.749617 188.421231 \r\nL 304.958529 188.374566 \r\nL 304.969525 188.657023 \r\nL 305.585267 188.083357 \r\nL 306.124042 190.912964 \r\nL 306.135037 190.284877 \r\nL 306.277977 189.188926 \r\nL 306.288973 189.52156 \r\nL 306.475894 190.660829 \r\nL 306.48689 191.574974 \r\nL 306.849738 190.650467 \r\nL 306.860733 190.064986 \r\nL 307.267563 189.281454 \r\nL 307.278558 188.796439 \r\nL 307.377517 188.90588 \r\nL 307.388512 189.129217 \r\nL 307.839324 184.398182 \r\nL 307.850319 185.414364 \r\nL 308.048236 184.96763 \r\nL 308.257149 184.519748 \r\nL 308.268144 184.756938 \r\nL 308.411085 183.746714 \r\nL 308.42208 184.112581 \r\nL 309.026827 184.742968 \r\nL 309.158772 182.215499 \r\nL 309.169767 183.41112 \r\nL 309.334698 179.428767 \r\nL 309.345693 176.3928 \r\nL 309.52162 168.319783 \r\nL 309.576597 174.686785 \r\nL 309.609583 170.752473 \r\nL 309.686551 171.965776 \r\nL 309.752523 169.259007 \r\nL 309.763519 172.132847 \r\nL 310.50021 160.456233 \r\nL 310.533197 165.747742 \r\nL 310.676137 159.638854 \r\nL 310.687132 162.011641 \r\nL 310.742109 164.60082 \r\nL 310.753105 162.355543 \r\nL 310.841068 162.9242 \r\nL 310.984008 161.491635 \r\nL 311.082966 156.563315 \r\nL 311.335861 163.916441 \r\nL 311.478801 173.260462 \r\nL 311.643732 165.273417 \r\nL 311.654727 166.426502 \r\nL 311.709704 169.759845 \r\nL 311.7207 167.80499 \r\nL 311.852644 170.716483 \r\nL 311.874635 170.70722 \r\nL 312.050562 166.403635 \r\nL 312.14952 167.259094 \r\nL 312.160516 165.484579 \r\nL 312.325447 161.528371 \r\nL 312.336442 161.721098 \r\nL 312.435401 162.288295 \r\nL 312.446396 162.189259 \r\nL 312.468387 160.74994 \r\nL 312.523364 164.397513 \r\nL 312.589336 165.614921 \r\nL 312.600332 164.336166 \r\nL 312.655309 163.910284 \r\nL 312.666304 164.46981 \r\nL 312.798249 160.020528 \r\nL 312.864221 160.613858 \r\nL 312.98517 163.742633 \r\nL 312.996166 162.676266 \r\nL 313.128111 160.410873 \r\nL 313.183088 162.426672 \r\nL 313.194083 162.246928 \r\nL 313.402996 160.765924 \r\nL 313.556931 152.260601 \r\nL 313.567927 152.26507 \r\nL 313.820821 158.944803 \r\nL 313.930775 155.551374 \r\nL 313.842812 159.398766 \r\nL 313.94177 155.745899 \r\nL 314.216655 167.164369 \r\nL 314.227651 164.833638 \r\nL 314.304618 164.57823 \r\nL 314.436563 159.674536 \r\nL 314.447559 161.791901 \r\nL 314.546517 159.971515 \r\nL 314.557513 160.859481 \r\nL 314.678462 160.814583 \r\nL 314.689457 161.270687 \r\nL 314.766425 159.879102 \r\nL 314.832398 156.648359 \r\nL 314.843393 156.733712 \r\nL 314.942351 156.366766 \r\nL 314.953347 156.432428 \r\nL 315.008324 151.238789 \r\nL 315.019319 152.252285 \r\nL 315.162259 152.897376 \r\nL 315.294204 157.333801 \r\nL 315.3052 156.192574 \r\nL 315.415154 159.623859 \r\nL 315.470131 155.749748 \r\nL 315.547098 157.808545 \r\nL 315.723025 161.628225 \r\nL 315.73402 159.496433 \r\nL 315.799993 160.37294 \r\nL 316.019901 155.836154 \r\nL 316.030896 156.601883 \r\nL 316.151845 156.579001 \r\nL 316.184832 152.565215 \r\nL 316.28379 156.184061 \r\nL 316.294786 154.766149 \r\nL 316.437726 156.59339 \r\nL 316.448721 156.3657 \r\nL 316.470712 154.466959 \r\nL 316.701615 155.964299 \r\nL 316.712611 155.728797 \r\nL 316.789579 158.830791 \r\nL 316.866546 157.633139 \r\nL 317.09745 156.162529 \r\nL 317.218399 151.49237 \r\nL 317.24039 152.68092 \r\nL 317.427312 146.804849 \r\nL 317.438307 147.678563 \r\nL 317.66921 140.90555 \r\nL 317.702197 141.976592 \r\nL 317.911109 143.650335 \r\nL 317.922105 143.413893 \r\nL 318.054049 137.689953 \r\nL 318.240971 140.846227 \r\nL 318.33993 138.82957 \r\nL 318.592824 146.274148 \r\nL 318.603819 146.165365 \r\nL 318.845718 138.907247 \r\nL 318.867709 139.589956 \r\nL 318.955672 136.96888 \r\nL 318.977663 138.393332 \r\nL 319.054631 135.398203 \r\nL 319.087617 138.073422 \r\nL 319.263543 134.494582 \r\nL 319.450465 138.961728 \r\nL 319.527433 138.485747 \r\nL 319.670373 134.545397 \r\nL 319.681368 135.571282 \r\nL 319.780327 136.014193 \r\nL 319.791322 136.955927 \r\nL 319.912272 135.723268 \r\nL 319.945258 132.729448 \r\nL 320.088198 132.711923 \r\nL 320.198152 135.004109 \r\nL 320.209148 134.908187 \r\nL 320.220143 135.037764 \r\nL 320.231138 134.012744 \r\nL 320.440051 133.716554 \r\nL 320.561 138.289582 \r\nL 320.791904 133.696148 \r\nL 320.802899 134.733095 \r\nL 320.945839 135.25874 \r\nL 321.066789 137.094505 \r\nL 321.25371 133.954699 \r\nL 321.264706 134.178272 \r\nL 321.462623 129.314181 \r\nL 321.649545 129.339964 \r\nL 321.66054 129.258841 \r\nL 321.759499 128.811576 \r\nL 321.880448 125.507124 \r\nL 321.935425 125.163628 \r\nL 321.946421 125.374772 \r\nL 322.023388 125.790313 \r\nL 322.034384 125.523029 \r\nL 322.100356 129.76483 \r\nL 322.221306 128.540567 \r\nL 322.232301 126.870889 \r\nL 322.485195 132.947552 \r\nL 322.573158 140.479225 \r\nL 322.584154 137.986649 \r\nL 322.639131 135.361567 \r\nL 322.661122 139.144439 \r\nL 322.771075 151.09705 \r\nL 322.804062 148.03681 \r\nL 322.826052 149.164611 \r\nL 322.925011 145.047565 \r\nL 323.04596 140.144366 \r\nL 323.133924 143.869026 \r\nL 323.254873 142.6 \r\nL 323.364827 143.062714 \r\nL 323.507767 151.631049 \r\nL 323.617721 146.664817 \r\nL 323.639712 149.414117 \r\nL 323.694689 152.579997 \r\nL 323.705684 148.920799 \r\nL 323.870615 150.400449 \r\nL 323.881611 148.280322 \r\nL 324.12351 156.630096 \r\nL 324.178487 154.940238 \r\nL 324.244459 151.479968 \r\nL 324.255454 152.286466 \r\nL 324.55233 145.480054 \r\nL 324.67328 149.4477 \r\nL 324.772238 149.044347 \r\nL 324.904183 142.492136 \r\nL 324.915178 144.244475 \r\nL 324.970155 143.271906 \r\nL 324.981151 144.754105 \r\nL 325.058118 144.906032 \r\nL 325.069114 145.966646 \r\nL 325.234045 146.34456 \r\nL 325.333003 150.723217 \r\nL 325.805806 142.48933 \r\nL 325.816801 140.930355 \r\nL 325.838792 141.311507 \r\nL 326.003723 140.975764 \r\nL 326.014718 142.272362 \r\nL 326.080691 137.639511 \r\nL 326.168654 140.294521 \r\nL 326.300599 134.506553 \r\nL 326.48752 133.079466 \r\nL 326.498516 133.897214 \r\nL 326.597474 132.676246 \r\nL 326.60847 134.077475 \r\nL 326.729419 134.322744 \r\nL 326.740414 132.950476 \r\nL 326.839373 133.706699 \r\nL 326.850368 133.368119 \r\nL 326.861364 135.102876 \r\nL 326.872359 135.005998 \r\nL 326.993309 136.803141 \r\nL 327.004304 135.335417 \r\nL 327.114258 133.577894 \r\nL 327.422129 137.758783 \r\nL 327.598056 129.137048 \r\nL 327.916922 132.411511 \r\nL 327.927918 130.348975 \r\nL 328.026876 135.964969 \r\nL 328.037872 135.646612 \r\nL 328.202803 136.170547 \r\nL 328.213798 137.569884 \r\nL 328.27977 134.997148 \r\nL 328.565651 136.581232 \r\nL 328.796554 132.893107 \r\nL 328.807549 134.210447 \r\nL 328.994471 136.28833 \r\nL 329.005467 135.702775 \r\nL 329.214379 133.402161 \r\nL 329.225375 134.240414 \r\nL 329.434287 132.526055 \r\nL 329.445283 133.033807 \r\nL 329.610214 136.885919 \r\nL 329.621209 135.720405 \r\nL 329.984057 135.957319 \r\nL 329.995053 140.712273 \r\nL 330.061025 139.912461 \r\nL 330.313919 140.407423 \r\nL 330.324915 139.790412 \r\nL 330.511836 139.632294 \r\nL 330.577809 145.92128 \r\nL 330.588804 145.460881 \r\nL 330.720749 144.172533 \r\nL 330.819707 144.406722 \r\nL 330.830703 143.464434 \r\nL 330.973643 142.820595 \r\nL 330.984638 141.308661 \r\nL 331.29251 138.713441 \r\nL 331.501422 139.225696 \r\nL 331.512418 139.760766 \r\nL 331.666353 140.544779 \r\nL 331.732326 135.728059 \r\nL 332.095174 132.792122 \r\nL 332.106169 133.478601 \r\nL 332.359063 132.786915 \r\nL 332.370059 129.141576 \r\nL 332.556981 129.052794 \r\nL 332.688925 126.572957 \r\nL 332.699921 126.652528 \r\nL 332.996796 120.16872 \r\nL 333.007792 120.359857 \r\nL 333.117746 122.471245 \r\nL 333.216704 125.666138 \r\nL 333.150732 121.30854 \r\nL 333.2277 125.383435 \r\nL 333.425617 121.975635 \r\nL 333.546566 125.019418 \r\nL 333.557562 123.877099 \r\nL 333.744484 119.596816 \r\nL 333.755479 119.616416 \r\nL 333.843442 121.071602 \r\nL 333.953396 120.020484 \r\nL 334.250272 126.459324 \r\nL 334.019369 119.79228 \r\nL 334.261267 126.25635 \r\nL 334.459185 122.465316 \r\nL 334.690088 121.964842 \r\nL 334.701083 123.473661 \r\nL 334.855019 117.393335 \r\nL 335.01995 119.84372 \r\nL 335.030945 119.34164 \r\nL 335.140899 119.316613 \r\nL 335.415784 118.08456 \r\nL 335.42678 119.769165 \r\nL 335.690669 116.21124 \r\nL 335.888586 108.421114 \r\nL 335.910577 108.474305 \r\nL 336.031527 109.670656 \r\nL 336.042522 109.380537 \r\nL 336.141481 104.21914 \r\nL 336.152476 104.724012 \r\nL 336.273425 104.846565 \r\nL 336.284421 104.804713 \r\nL 336.361389 101.120875 \r\nL 336.372384 101.522414 \r\nL 336.471343 107.867904 \r\nL 336.482338 107.836307 \r\nL 336.735232 103.042213 \r\nL 336.823195 110.254313 \r\nL 336.834191 107.830239 \r\nL 336.911158 112.462678 \r\nL 337.021112 123.310694 \r\nL 337.032108 120.258934 \r\nL 337.21903 112.518157 \r\nL 337.230025 113.549863 \r\nL 337.427942 120.846558 \r\nL 337.515905 118.41883 \r\nL 337.603869 113.322346 \r\nL 337.680836 109.730295 \r\nL 337.713823 112.08063 \r\nL 337.834772 110.49462 \r\nL 337.845767 105.359664 \r\nL 337.91174 108.802968 \r\nL 338.131648 97.105666 \r\nL 338.164634 100.761212 \r\nL 338.296579 119.369148 \r\nL 338.417528 109.721174 \r\nL 338.472505 110.083008 \r\nL 338.571464 113.783068 \r\nL 338.637436 116.332161 \r\nL 338.659427 113.306414 \r\nL 338.736395 113.726926 \r\nL 338.791372 117.544588 \r\nL 338.901326 109.397787 \r\nL 338.923316 110.698466 \r\nL 339.000284 113.799014 \r\nL 339.01128 105.204237 \r\nL 339.198201 101.900575 \r\nL 339.220192 102.668049 \r\nL 339.231188 103.295024 \r\nL 339.286165 100.907823 \r\nL 339.29716 101.902427 \r\nL 339.396119 99.39845 \r\nL 339.418109 99.755144 \r\nL 339.429105 95.280282 \r\nL 339.616027 96.353211 \r\nL 339.627022 96.317646 \r\nL 339.769962 90.558698 \r\nL 339.802948 92.006599 \r\nL 339.857925 93.138737 \r\nL 339.868921 91.494377 \r\nL 339.967879 89.402244 \r\nL 340.033852 91.851168 \r\nL 340.044847 91.102193 \r\nL 340.143806 89.626504 \r\nL 340.154801 90.016398 \r\nL 340.25376 91.097756 \r\nL 340.264755 90.605102 \r\nL 340.528645 82.1297 \r\nL 340.638599 82.938158 \r\nL 340.693576 84.599165 \r\nL 340.704571 83.023245 \r\nL 340.858507 79.494501 \r\nL 341.078415 63.314448 \r\nL 341.133392 64.55495 \r\nL 341.144387 68.596132 \r\nL 341.177373 61.661758 \r\nL 341.210359 64.184496 \r\nL 341.265336 55.125361 \r\nL 341.287327 68.586831 \r\nL 341.298323 64.511604 \r\nL 341.3533 60.749754 \r\nL 341.364295 64.545267 \r\nL 341.37529 67.95897 \r\nL 341.397281 61.978014 \r\nL 341.463254 65.323504 \r\nL 341.595198 56.699332 \r\nL 341.63918 54.741336 \r\nL 341.727143 59.397973 \r\nL 341.804111 53.904168 \r\nL 341.859088 56.554048 \r\nL 341.881079 55.057592 \r\nL 341.991033 60.116078 \r\nL 342.078996 52.918214 \r\nL 342.111982 54.722098 \r\nL 342.166959 57.90315 \r\nL 342.177955 54.767004 \r\nL 342.320895 49.388241 \r\nL 342.430849 36.690295 \r\nL 342.342886 49.699846 \r\nL 342.463835 37.261566 \r\nL 342.562793 36.75194 \r\nL 342.573789 44.494329 \r\nL 342.61777 36.614839 \r\nL 342.672747 37.78381 \r\nL 342.683743 38.48995 \r\nL 342.892655 26.150592 \r\nL 342.903651 26.636423 \r\nL 342.991614 41.109779 \r\nL 343.0246 38.45416 \r\nL 343.079577 30.915179 \r\nL 343.101568 34.883048 \r\nL 343.266499 17.083636 \r\nL 343.299485 18.464582 \r\nL 343.398444 20.629662 \r\nL 343.497402 37.230994 \r\nL 343.541384 36.947294 \r\nL 343.563375 31.752282 \r\nL 343.607356 46.106259 \r\nL 343.662333 39.867104 \r\nL 343.673329 46.630304 \r\nL 343.684324 46.490779 \r\nL 343.69532 49.776473 \r\nL 343.761292 40.807475 \r\nL 343.926223 26.964694 \r\nL 343.783283 42.85732 \r\nL 343.937218 27.192694 \r\nL 344.036177 27.189871 \r\nL 344.146131 39.079825 \r\nL 344.157126 35.235792 \r\nL 344.212103 43.523555 \r\nL 344.223099 42.446336 \r\nL 344.278076 45.346455 \r\nL 344.289071 43.281712 \r\nL 344.366039 39.214813 \r\nL 344.399025 39.41688 \r\nL 344.585947 67.103219 \r\nL 344.596942 63.043052 \r\nL 344.618933 59.987462 \r\nL 344.640924 53.336788 \r\nL 344.717892 65.763782 \r\nL 344.739882 70.051839 \r\nL 344.772869 59.001488 \r\nL 344.794859 67.382653 \r\nL 344.871827 54.871559 \r\nL 344.904813 55.391786 \r\nL 345.201689 85.912789 \r\nL 345.212685 110.211681 \r\nL 345.311643 98.235816 \r\nL 345.421597 92.175796 \r\nL 345.344629 114.614084 \r\nL 345.432593 93.203627 \r\nL 345.443588 94.175649 \r\nL 345.465579 90.139726 \r\nL 345.476574 82.516926 \r\nL 345.531551 90.47581 \r\nL 345.575533 86.548302 \r\nL 345.586528 86.394825 \r\nL 345.608519 83.455506 \r\nL 345.718473 93.259437 \r\nL 345.740464 95.063956 \r\nL 345.751459 91.701854 \r\nL 345.828427 113.597478 \r\nL 345.850418 104.109462 \r\nL 345.949376 114.454938 \r\nL 345.971367 108.532734 \r\nL 345.982363 103.937723 \r\nL 346.048335 111.156558 \r\nL 346.147294 109.335635 \r\nL 346.312224 132.117049 \r\nL 346.334215 126.868455 \r\nL 346.664077 92.340844 \r\nL 346.741045 94.935434 \r\nL 346.785027 107.067725 \r\nL 346.818013 97.242921 \r\nL 346.87299 100.436018 \r\nL 346.894981 96.998697 \r\nL 347.004935 83.957795 \r\nL 347.01593 85.332088 \r\nL 347.103893 91.06148 \r\nL 347.202852 87.608911 \r\nL 347.224843 93.845441 \r\nL 347.235838 92.258665 \r\nL 347.345792 86.278555 \r\nL 347.367783 89.458969 \r\nL 347.455746 91.244081 \r\nL 347.70864 108.566198 \r\nL 347.774613 101.51396 \r\nL 347.785608 102.419835 \r\nL 347.939544 114.588103 \r\nL 348.016511 113.323301 \r\nL 348.104475 104.551631 \r\nL 348.137461 106.292284 \r\nL 348.25841 108.260125 \r\nL 348.269405 112.823642 \r\nL 348.335378 102.710375 \r\nL 348.346373 103.12848 \r\nL 348.423341 97.854346 \r\nL 348.566281 93.21758 \r\nL 348.588272 95.287539 \r\nL 348.66524 98.893319 \r\nL 348.720217 100.215588 \r\nL 348.731212 98.252858 \r\nL 348.786189 97.787263 \r\nL 348.797185 98.037306 \r\nL 348.962116 89.43427 \r\nL 348.973111 92.905604 \r\nL 349.127047 86.87851 \r\nL 349.138042 84.976325 \r\nL 349.182024 89.459716 \r\nL 349.193019 87.934931 \r\nL 349.313968 88.183315 \r\nL 349.368945 96.517318 \r\nL 349.434918 96.048315 \r\nL 349.566863 89.81016 \r\nL 349.665821 89.969988 \r\nL 349.797766 98.120206 \r\nL 349.918715 92.186908 \r\nL 349.995683 93.435848 \r\nL 350.061656 93.05458 \r\nL 350.226587 87.013384 \r\nL 350.248577 89.228013 \r\nL 350.512467 108.755581 \r\nL 350.578439 100.123142 \r\nL 350.644412 104.905236 \r\nL 350.765361 100.333655 \r\nL 350.776356 104.546166 \r\nL 350.798347 98.859811 \r\nL 350.897306 98.54669 \r\nL 350.985269 100.738347 \r\nL 350.996264 98.48314 \r\nL 351.183186 105.001739 \r\nL 351.194182 103.454037 \r\nL 351.216172 104.746371 \r\nL 351.315131 104.530091 \r\nL 351.43608 113.123095 \r\nL 351.447076 111.616229 \r\nL 351.458071 113.067325 \r\nL 351.546034 105.269817 \r\nL 351.612007 106.482943 \r\nL 351.688975 107.611871 \r\nL 351.69997 109.454339 \r\nL 351.84291 109.184057 \r\nL 351.853906 104.439884 \r\nL 352.040827 101.975766 \r\nL 352.051823 102.863383 \r\nL 352.062818 103.314513 \r\nL 352.084809 99.771045 \r\nL 352.150781 100.350841 \r\nL 352.161777 98.983896 \r\nL 352.348698 97.585189 \r\nL 352.425666 100.409444 \r\nL 352.590597 102.410562 \r\nL 352.667565 107.052427 \r\nL 352.788514 106.313235 \r\nL 352.79951 104.402314 \r\nL 352.876478 107.455294 \r\nL 352.898468 106.314292 \r\nL 352.920459 111.11394 \r\nL 353.019418 109.375066 \r\nL 353.096386 110.678714 \r\nL 353.107381 110.383563 \r\nL 353.195344 117.387027 \r\nL 353.327289 114.350177 \r\nL 353.470229 115.011735 \r\nL 353.536202 117.907427 \r\nL 353.569188 115.477804 \r\nL 353.624165 117.38886 \r\nL 353.63516 122.491982 \r\nL 353.734119 117.236129 \r\nL 353.745114 112.795889 \r\nL 353.833077 119.699934 \r\nL 353.844073 119.276131 \r\nL 353.965022 128.274584 \r\nL 354.030995 131.200151 \r\nL 354.085972 125.930324 \r\nL 354.096967 126.725418 \r\nL 354.217916 134.765225 \r\nL 354.228912 129.081968 \r\nL 354.305879 131.158979 \r\nL 354.338866 124.967407 \r\nL 354.404838 125.439605 \r\nL 354.602755 130.401285 \r\nL 354.833659 125.070163 \r\nL 354.844654 126.391888 \r\nL 354.866645 124.943577 \r\nL 354.99859 134.423634 \r\nL 355.064562 132.76876 \r\nL 355.196507 136.91676 \r\nL 355.262479 136.290984 \r\nL 355.361438 134.85861 \r\nL 355.394424 130.373331 \r\nL 355.603337 133.990391 \r\nL 355.724286 122.43869 \r\nL 355.746277 126.659421 \r\nL 355.812249 121.062326 \r\nL 355.911208 120.045435 \r\nL 355.922203 121.247781 \r\nL 356.054148 114.508906 \r\nL 356.208083 121.363246 \r\nL 356.351024 113.409139 \r\nL 356.362019 115.746379 \r\nL 356.515955 120.885907 \r\nL 356.614913 120.121058 \r\nL 356.647899 124.375128 \r\nL 357.098711 115.476776 \r\nL 357.120702 115.090392 \r\nL 357.131697 116.849288 \r\nL 357.329614 108.801884 \r\nL 357.648481 116.57373 \r\nL 357.659476 115.018413 \r\nL 357.758435 109.918903 \r\nL 357.813412 111.256547 \r\nL 357.91237 111.084111 \r\nL 358.022324 107.134404 \r\nL 358.03332 107.639555 \r\nL 358.264223 117.255574 \r\nL 358.363182 119.057733 \r\nL 358.550103 121.092345 \r\nL 358.561099 120.36019 \r\nL 358.693044 118.155283 \r\nL 358.715034 118.699132 \r\nL 358.835984 121.095171 \r\nL 358.912952 121.068259 \r\nL 359.01191 120.686528 \r\nL 359.022906 121.124077 \r\nL 359.209827 118.43347 \r\nL 359.231818 119.159861 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 39.65 224.64 \r\nL 39.65 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 374.45 224.64 \r\nL 374.45 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 39.65 224.64 \r\nL 374.45 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 39.65 7.2 \r\nL 374.45 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pbb88fb4f50\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import polar_pla as pla\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# read in time series into temporary list\n",
    "temp = []\n",
    "f = open('DataSets/eth.csv', 'r')\n",
    "for line in f:\n",
    "    temp.append(float(line))\n",
    "\n",
    "# run bottom up piecewise linear approximation on that list and store processed values\n",
    "data = pla.bottom_up_pla(temp, 1000)\n",
    "#print(data)\n",
    "# set the sequence length (the number of trends we look at to predict the next) and the train to test ratio\n",
    "seq_length = 12\n",
    "train_proportion = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([32, 2])\ntorch.Size([5, 2])\n"
    }
   ],
   "source": [
    "# segment the data into input output pairs that we will use to train the model\n",
    "def sliding_window(data):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_numpy = []\n",
    "    out_numpy = []\n",
    "    for i in range(0, len(data)-seq_length, 2):\n",
    "        #inputs.append(data[i:(i+seq_length)]) # the next n are the input\n",
    "        #outputs.append(data[i+seq_length:i+seq_length+2]) # and the one after that is the output\n",
    "        input_numpy.append(np.array(data[i:(i+seq_length)]).reshape(int(seq_length/2),2))\n",
    "        out_numpy.append(np.array(data[i+seq_length:i+seq_length+2]))\n",
    "        \n",
    "    return Variable(torch.Tensor(input_numpy)), Variable(torch.Tensor(out_numpy))\n",
    "\n",
    "# convert data to tensor, and apply dataloader\n",
    "total_data_input, total_data_output = sliding_window(data)\n",
    "train_size = int(len(total_data_input)*train_proportion)\n",
    "\n",
    "training_data_input = torch.narrow(total_data_input, 0, 0, train_size)\n",
    "training_data_output = torch.narrow(total_data_output, 0, 0, train_size)\n",
    "\n",
    "testing_data_input = torch.narrow(total_data_input, 0, train_size, len(total_data_input) - train_size)\n",
    "testing_data_output = torch.narrow(total_data_output, 0, train_size, len(total_data_input) - train_size)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(training_data_input, training_data_output)\n",
    "test = torch.utils.data.TensorDataset(testing_data_input, testing_data_output)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=32, shuffle=False)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "\n",
    "for i, (inputs, outputs) in enumerate(trainset):\n",
    "   print(outputs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True\n",
    "                            )\n",
    "                            \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        h_out = h_out.view(-1, self.hidden_size)\n",
    "        out = self.fc(h_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_classes = 2\n",
    "input_size = 2\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 0, loss: 0.41945568\nEpoch: 100, loss: 0.00601490\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1972/314173429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1972/1879112018.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Propagate input through LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mula\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mh_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    680\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    681\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (trainX, trainY) in enumerate(trainset):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm(trainX)\n",
    "\n",
    "        loss = criterion(outputs, trainY)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.8f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 2])\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1972/274326303.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1972/2420759075.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Propagate input through LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mula\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mh_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    618\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                            ):\n\u001b[1;32m--> 620\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0;32m    622\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mexpected_input_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_input_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    202\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m    203\u001b[0m                     expected_input_dim, input.dim()))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    X, Y = [], []\n",
    "    X2, Y2 = [], []\n",
    "    total_loss = 0\n",
    "    for data in test:\n",
    "        inputs, labels = data\n",
    "        print(inputs.shape)\n",
    "        output = lstm(inputs)\n",
    "        lstm.zero_grad()\n",
    "        total_loss += F.mse_loss(output, labels).item()\n",
    "        #output = output[0]\n",
    "        print(output.shape)\n",
    "        print(labels.shape)\n",
    "        if output[0][0] < 0 and labels[0][0] < 0 or output[0][0] > 0 and labels[0][0] > 0:\n",
    "            correct += 1\n",
    "            print(output[0],labels[0])\n",
    "    print(f'Directional Accuracy: {correct*100/len(test)}, Average Loss: {total_loss/len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitf0b83480d49542e399c9eede80ba5b6b",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}